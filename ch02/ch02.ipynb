{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install spacy\n",
    "!python -m spacy download pt_core_news_lg\n",
    "!python -m pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.pt.Portuguese at 0x7fb2cf4a0dd0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_lg\")\n",
    "nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Eu', 'estou', 'indo', 'para', 'Brasília']\n"
     ]
    }
   ],
   "source": [
    "## tokenization\n",
    "\n",
    "\n",
    "doc = nlp(u'Eu estou indo para Brasília')\n",
    "print([w.text for w in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eu, -> eu\n",
      "estou, -> estar\n",
      "indo, -> ir\n",
      "para, -> para\n",
      "Brasília, -> Brasília\n"
     ]
    }
   ],
   "source": [
    "## lemmatization\n",
    "\n",
    "for token in doc:\n",
    "    print(f'{token.text}, -> {token.lemma_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eu -> PRON\n",
      "estou -> AUX\n",
      "indo -> VERB\n",
      "para -> ADP\n",
      "Brasília -> PROPN\n"
     ]
    }
   ],
   "source": [
    "## Part-of-Speech Tagging\n",
    "\n",
    "for token in doc:\n",
    "    print(f'{token.text} -> {token.tag_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ir', 'para']\n",
      "['indo', 'para']\n"
     ]
    }
   ],
   "source": [
    "print([token.lemma_ for token in doc if token.tag_ == 'VERB' or token.tag_ == 'ADP'])\n",
    "print([token.text for token in doc if token.tag_ == 'VERB' or token.tag_ == 'ADP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eu PRON nsubj\n",
      "estou AUX aux\n",
      "indo VERB ROOT\n",
      "para ADP case\n",
      "Brasília PROPN obl\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indo PRON nsubj\n",
      "indo AUX aux\n",
      "indo VERB ROOT\n",
      "Brasília ADP case\n",
      "indo PROPN obl\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.head.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Petit Prince"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLE\n",
      "\n",
      "tinha\n",
      "vi\n",
      "Representava\n",
      "engolia\n",
      "Dizia\n",
      "engolem\n",
      "mastigar\n",
      "podem\n",
      "mover-se\n",
      "dormem\n",
      "\n",
      "2099 -> verbos\n"
     ]
    }
   ],
   "source": [
    "# Program to read the entire file using read() function\n",
    "file = open(\"petit.txt\", \"r\")\n",
    "content = file.read()\n",
    "content = nlp(content)\n",
    "\n",
    "\n",
    "#for word in content:\n",
    "#    if word.tag_=='VERB':\n",
    "#        print(word.text)\n",
    "\n",
    "print('SAMPLE')\n",
    "print('')\n",
    "\n",
    "verbs = ([word.text for word in content if word.tag_=='VERB'])\n",
    "for i in range(0, 10):\n",
    "    print(verbs[i])\n",
    "  \n",
    "print('')\n",
    "\n",
    "print(f'{len(verbs)} -> verbos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ter\n",
      "ver\n",
      "Representava\n",
      "engoliar\n",
      "Dizia\n",
      "engoler\n",
      "mastigar\n",
      "poder\n",
      "mover se\n",
      "dormir\n",
      "\n"
     ]
    }
   ],
   "source": [
    "verbs_lemma = ([word.lemma_ for word in content if word.tag_=='VERB'])\n",
    "for i in range(0, 10):\n",
    "    print(verbs_lemma[i])\n",
    "  \n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['voar', 'Maceió']\n"
     ]
    }
   ],
   "source": [
    "test = nlp(u'Eu vou voar para Maceió')\n",
    "print([t.lemma_ for t in test if t.tag_=='VERB' or t.dep_=='obl'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brasília 385\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    if token.ent_type != 0:\n",
    "        print(token.text, token.ent_type)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
