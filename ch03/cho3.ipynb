{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install spacy\n",
    "!python -m spacy download pt_core_news_lg\n",
    "!python -m pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Chegou, uma, tempestada, forte, na, praia]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_lg\")\n",
    "\n",
    "doc = nlp(u'Chegou uma tempestada forte na praia')\n",
    "[doc[i] for i in range(len(doc))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating over a Token’s Syntactic Children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([madura], [uma, madura])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2 = nlp(u'Eu quero uma maçã madura')\n",
    "[w for w in doc2[3].rights], [w for w in doc2[3].children]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Eu, quero, uma, maçã, .]\n",
      "[Hoje, é, dia, de, comer, frutas, !]\n"
     ]
    }
   ],
   "source": [
    "doc3 = nlp(u'Eu quero uma maçã. Hoje é dia de comer frutas!')\n",
    "\n",
    "for sent in doc3.sents:\n",
    "    print([sent[i] for i in range(len(sent))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sent in enumerate(doc.sents):\n",
    "    if i==0 and sent[0].pos_ == 'PRON':\n",
    "        print('A primeira sentença começa com um pronome!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = 0\n",
    "\n",
    "for sent in doc3.sents:\n",
    "    if sent[len(sent)-3].pos_ == 'VERB':\n",
    "        counter += 1\n",
    "\n",
    "counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The doc.noun_chunks Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A vida\n",
      "Eu\n",
      "essa mesa\n",
      "ela\n"
     ]
    }
   ],
   "source": [
    "doc4 = nlp(u'A vida é complicada. Eu comprei essa mesa ontem e ela está quebrada.')\n",
    "\n",
    "for chunk in doc4.noun_chunks:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doc.sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[A, vida, é, complicada, .]\n",
      "[Eu, comprei, essa, mesa, ontem, e, ela, está, quebrada, .]\n"
     ]
    }
   ],
   "source": [
    "for sent in doc4.sents:\n",
    "    print([sent[i] for i in range(len(sent))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 A\n",
      "1 Eu\n",
      "First Element Second Sentence: Eu\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "here, i'm proving that, the first element\n",
    "from the second sentence is a PRON.\n",
    "'''\n",
    "\n",
    "for i, sent in enumerate(doc4.sents):\n",
    "    print(i, sent[0])\n",
    "\n",
    "    if i == 1 and sent[0].pos_ == 'PRON':\n",
    "        print(f'First Element Second Sentence: {sent[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words that are adjectives: 2s\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for i, sent in enumerate(doc4.sents):\n",
    "    #print(i, sent[-2])\n",
    "    #print(len(sent))\n",
    "    if sent[len(sent)-2].pos_ == 'ADJ':\n",
    "        #print('ADJ')\n",
    "        #print(sent[-2])\n",
    "        #print(len(sent[len(sent)-2]))\n",
    "        counter +=1\n",
    "\n",
    "print(f'Words that are adjectives: {counter}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'here'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Reason why we use -2, \n",
    "to get the last value\n",
    "\"\"\"\n",
    "\n",
    "t =['text', 'here', '.']\n",
    "t[-2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
